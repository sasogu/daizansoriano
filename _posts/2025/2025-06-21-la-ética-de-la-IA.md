---
title: "Harari, la ética de la IA y la mirada budista: deontología, utilitarismo y compasión en diálogo"
subtitle: ""
date: 2025-06-21 09:00:00 +0100
categories: ["Etica"]
tags: ["IA"]
---

Estoy leyendo el último libro de ***Yuval Noah Harari***, *Nexus: Una breve historia de las redes de información desde la Edad de Piedra hasta la IA*. En él, Harari explora cómo la información ha tejido la historia humana y cómo la irrupción de la inteligencia artificial abre un capítulo sin precedentes. Entre sus reflexiones, dedica un espacio a un asunto que como budista me parece clave: qué principios éticos deberían guiar a la IA. Este texto no es un resumen del libro, sino un diálogo con sus propuestas desde la práctica del budismo soto zen.

Harari menciona dos grandes corrientes que han marcado la filosofía moral occidental: la deontología y el utilitarismo. A partir de ahí, advierte de sus límites y propone un marco más flexible. Aquí intento explicar esos enfoques con ejemplos sencillos y ponerlos en conversación con la ética budista, para ver semejanzas, diferencias y, sobre todo, qué puede servirnos hoy al diseñar y usar sistemas de IA.

### Deontología: el deber por encima de todo

La deontología, cuyo representante más conocido es Immanuel Kant, parte de la idea de que existen principios morales universales que deben cumplirse siempre, sin importar las consecuencias. Dicho simplemente: hay cosas que están mal, «pase lo que pase». Ejemplos clásicos son «no matarás» o «no mentirás». En la vida diaria, se parece a respetar un semáforo en rojo, aunque la calle esté vacía: la norma protege algo valioso y no depende de cómo te sientas hoy.

Trasladado a la IA, esto implicaría programar sistemas con reglas fijas. Por ejemplo: un asistente que nunca invente datos (no mentir), un vehículo autónomo que nunca exceda la velocidad, o un algoritmo médico que nunca comparta información sensible. La ventaja es la claridad: sabemos a qué atenernos. El riesgo es la rigidez: el mundo es cambiante, y una regla absoluta puede chocar con situaciones límite. ¿Qué pasa si «no mentir» expone a alguien a un daño grave? ¿O si respetar una regla al pie de la letra empeora un problema en vez de aliviarlo?

Harari advierte que esta rigidez puede causar efectos no deseados. Un sistema que actúa sin considerar el contexto corre el riesgo de hacer daño por obedecer demasiado bien. En ética, la intención importa, pero también el resultado y la situación concreta.

### Utilitarismo: el bien medido en resultados

El utilitarismo mide lo correcto por sus consecuencias: la acción buena es la que produce el mayor beneficio para el mayor número de personas. En la vida diaria, lo reconocemos cuando alguien dice: «no es ideal, pero es lo mejor para la mayoría». Trasladado a la IA, imaginaríamos algoritmos que calculan cómo maximizar el bienestar global: asignar recursos sanitarios, organizar el tráfico, o priorizar contenidos educativos que más ayuden.

Su fuerza es la practicidad y la mirada a gran escala. Su sombra es que, si solo importa el resultado agregado, podemos justificar medios dañinos, sacrificar a minorías o ignorar valores como la justicia o la dignidad. Una IA obsesionada con «la eficiencia» podría tomar decisiones frías y deshumanizadas, aunque mejore algunas estadísticas.

### El marco que propone Harari

En Nexus, Harari no se queda con una sola postura. Propone un enfoque combinado que incluya, al menos, tres elementos:

- Benevolencia activa: que la IA no se limite a obedecer, sino que busque hacer el bien. Es la diferencia entre «no hacer daño» y «cuidar».
- Sabiduría socrática: reconocer límites, ser capaz de decir «no sé» y actuar con cautela ante la incertidumbre. La humildad es una virtud técnica y moral.
- Regulación independiente: instituciones humanas que supervisen, corrijan y auditen los sistemas para evitar abusos y sesgos. Ninguna tecnología debería evaluarse a sí misma en solitario.

La idea de fondo es que ninguna ética por sí sola basta para la complejidad de un mundo donde también deciden inteligencias no humanas. Necesitamos una aproximación híbrida, adaptativa y vigilada.

### La ética budista: contexto, intención y despertar

¿Qué puede aportar el budismo en este sentido? Desde mi punto de vista ofrece un marco que, sin coincidir exactamente con deontología o utilitarismo, recoge aspectos de ambas dentro de una visión distinta. Tres claves:

- Contextualidad. Las acciones se valoran según la situación concreta y la red de relaciones implicadas. Nada existe aislado; todo está interconectado. Mirar el contexto evita lecturas rígidas y nos recuerda que un mismo acto puede tener efectos muy diferentes según las condiciones.

- Intencionalidad. Lo decisivo no es solo el acto, sino la motivación que lo impulsa. ¿Nace de la codicia, la aversión o la ignorancia (los «tres venenos»)? ¿O surge de la compasión y la sabiduría? En términos modernos, podríamos hablar de nuestras tendencias kármikas: hábitos mentales y emocionales que condicionan cómo percibimos y actuamos. En IA, esto inspira a hacer visibles las intenciones del sistema: qué optimiza, qué prioriza y con qué límites.

- Orientación al despertar. El objetivo último del budismo no es cumplir deberes ni maximizar placeres, sino reducir el sufrimiento y favorecer la liberación de todos los seres. Los preceptos del budismo, como no dañar, no mentir, no robar, etc., son entrenamientos, no castigos ni dogmas. Nos ayudan a cultivar compasión (karuna) y sabiduría (prajñā) en coherencia con el fin. En este marco, una acción «eficiente» pero guiada por la codicia o la indiferencia no se considera plenamente correcta.

### Ejemplos cotidianos

Vehículos autónomos. La deontología diría «no exceder la velocidad, nunca». El utilitarismo podría permitir excepciones si eso reduce accidentes en conjunto. La mirada budista añadiría: ¿qué intención guía al sistema? ¿Busca genuinamente no dañar y cuidar? ¿Cómo pondera riesgos para personas vulnerables, peatones, ciclistas y menores, en contextos reales, no ideales?

Diagnóstico médico con IA. Un enfoque utilitarista prioriza aumentar aciertos globales; el deontológico insiste en no violar la confidencialidad. La ética budista recuerda que la relación clínica es también un vínculo humano: transparencia, posibilidad de decir «no sé» y derivar, y atención especial a quienes pueden quedar fuera del «promedio» (enfermedades raras, minorías no representadas en los datos).

Moderación de contenidos. Maximizar «bienestar medio» podría silenciar voces minoritarias para evitar conflictos. Un enfoque budista invitaría a proteger la dignidad y crear espacios de diálogo, con reglas claras y aplicadas, con sensibilidad al contexto, además de rendición de cuentas ante la comunidad.

Educación. Un tutor virtual puede optimizar resultados de exámenes, pero ¿qué pasa con la curiosidad, la paciencia o la confianza del alumnado? La benevolencia activa pide que la IA cuide no solo el rendimiento inmediato, sino el bienestar profundo y el aprendizaje significativo a largo plazo.

### Principios operativos para una IA compasiva y prudente

No basta con bellas palabras; hacen falta guías prácticas. Propongo esta síntesis inspirada por Harari y por la ética budista:

- Mínimo daño: priorizar no dañar, especialmente a personas y colectivos vulnerables. Cuando haya incertidumbre, elegir la opción más cuidadosa.
- Intención explícita: declarar qué optimiza el sistema, qué límites tiene y qué nunca hará. Evitar objetivos opacos.
- Humildad técnica: capacidad real de decir «no sé», pedir ayuda humana y detenerse. Los sistemas deben poder fallar con seguridad.
- Contexto e interdependencia: evaluar efectos más allá del caso individual: en el ecosistema social, cultural y ambiental.
- Medios hábiles (upaya): adaptar la respuesta al caso sin traicionar los valores de fondo. Flexibilidad con brújula ética.
- Rendición de cuentas: auditorías independientes, trazabilidad de decisiones y vías de reparación cuando haya daños.
- Formación y práctica: equipos que cultiven atención plena, escucha y ética aplicada. La calidad humana del equipo se refleja en el código de programación de las IA.

### Un diálogo posible

Si aplicamos esta mirada al problema que plantea Harari, podríamos decir que la deontología aporta claridad de principios, pero necesita flexibilidad; el utilitarismo aporta atención a resultados, pero necesita cuidar los medios y las motivaciones; y la ética budista propone sostener juntas las tres dimensiones —principios, consecuencias e intenciones— orientadas a la disminución del sufrimiento.

Un sistema de IA inspirado en este espíritu no solo seguiría normas ni solo optimizaría métricas: leería el contexto, cuidaría la intención detrás de cada decisión y priorizaría el bienestar profundo, no solo la satisfacción inmediata. No se trata de máquinas «buenas» por decreto, sino de tecnologías inscritas en comunidades que practican la responsabilidad, la transparencia y el cuidado.

----
La IA no será mejor que las comunidades que la desarrollan. Si creamos marcos donde la verdad se contrasta, el poder rinde cuentas y el error se repara, la técnica puede ponerse al servicio de la dignidad. Esto se resume en un compromiso práctico: reducir el daño, cuidar a quienes quedan fuera del promedio y aprender juntos a cada paso.

No programamos solo código: nos programamos a nosotros y nosotras cada vez que elegimos cómo usarlo. Cuando una mente sabe parar, escuchar y cuidar, la tecnología se vuelve un medio hábil, no un obstáculo. Mirar el sufrimiento de frente y responder de la manera más adecuada posible en cada situación es una ética que necesitamos aplicar también a la IA.
